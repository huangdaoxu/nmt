{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jieba\n",
    "import nltk\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/b5/lmshx9x92hd8brq1_0jwgsrm0000gn/T/jieba.cache\n",
      "Loading model cost 0.719 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "def handle_datum2017(source_path, result_path):\n",
    "    for i in range(1, 21, 1):\n",
    "        with open(os.path.join(result_path, \"Book_en.txt\"), mode=\"a+\") as f_en:\n",
    "            with open(os.path.join(source_path, \"Book{}_en.txt\".format(i)), mode=\"r\") as f:\n",
    "                for line in f.readlines():\n",
    "                    text = nltk.word_tokenize(line)\n",
    "                    f_en.write(\" \".join(text) + \"\\n\")\n",
    "        \n",
    "        with open(os. path.join(result_path, \"Book_cn.txt\"), mode=\"a+\") as f_cn:\n",
    "            with open(os.path.join(source_path, \"Book{}_cn.txt\".format(i)), mode=\"r\") as f:\n",
    "                for line in f.readlines():\n",
    "                    text = jieba.cut(line.replace(\"\\n\", \"\"), cut_all=False)\n",
    "                    f_cn.write(\" \".join(text) + \"\\n\")\n",
    "                    \n",
    "handle_datum2017(\"/Users/hdx/data/datum2017\", \"./data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    return data\n",
    "\n",
    "source_path = 'data/Book_en.txt'\n",
    "target_path = 'data/Book_cn.txt'\n",
    "source_text = load_data(source_path)\n",
    "target_text = load_data(target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Brief Stats\n",
      "* number of unique words in English sample sentences: 190061\n",
      "\n",
      "* English sentences\n",
      "\t- number of sentences: 1000005\n",
      "\t- avg. number of words in a sentence: 27.719305403472983\n",
      "\n",
      "* Chinese sentences\n",
      "\t- number of sentences: 1000005 [data integrity check / should have the same number]\n",
      "\t- avg. number of words in a sentence: 24.891550542247288\n",
      "\n",
      "* Sample sentences range from 0 to 2\n",
      "[1-th] sentence\n",
      "\tEN: 2 . The Committee elected Syed Amjad Ali as Chairman and Mr. Peter Gregg as Vice-Chairman by acclamation .\n",
      "\tCN: 2   .   委员会   以   鼓掌   方式   推选   赛义德 · 阿姆 贾德 · 阿里   为主   席   ,   彼得 · 格雷格   先生   为   副 主席   。    \n",
      "\n",
      "[2-th] sentence\n",
      "\tEN: Mr. Henrik Amneus also provided valuable assistance as acting Vice-Chairman responsible for leading key sessions of the drafting committee .\n",
      "\tCN:   同时   ,   亨 里克 · 安纽斯   先生   作为   主持   起草   委员会   各个   主要   届   会   的   代理   副 主席   也   提供   了   宝贵   的   援助   。    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "print('Dataset Brief Stats')\n",
    "print('* number of unique words in English sample sentences: {}'.format(len(Counter(source_text.split()))))\n",
    "print()\n",
    "\n",
    "english_sentences = source_text.split('\\n')\n",
    "print('* English sentences')\n",
    "print('\\t- number of sentences: {}'.format(len(english_sentences)))\n",
    "print('\\t- avg. number of words in a sentence: {}'.format(np.average([len(sentence.split()) for sentence in english_sentences])))\n",
    "print()\n",
    "\n",
    "chinese_sentences = target_text.split('\\n')\n",
    "print('* Chinese sentences')\n",
    "print('\\t- number of sentences: {} [data integrity check / should have the same number]'.format(len(chinese_sentences)))\n",
    "print('\\t- avg. number of words in a sentence: {}'.format(np.average([len(sentence.split()) for sentence in chinese_sentences])))\n",
    "print()\n",
    "\n",
    "sample_sentence_range = (0, 2)\n",
    "side_by_side_sentences = list(zip(english_sentences, chinese_sentences))[sample_sentence_range[0]:sample_sentence_range[1]]\n",
    "print('* Sample sentences range from {} to {}'.format(sample_sentence_range[0], sample_sentence_range[1]))\n",
    "\n",
    "for index, sentence in enumerate(side_by_side_sentences):\n",
    "    en_sent, cn_sent = sentence\n",
    "    print('[{}-th] sentence'.format(index+1))\n",
    "    print('\\tEN: {}'.format(en_sent))\n",
    "    print('\\tCN: {}'.format(cn_sent))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pickle\n",
    "\n",
    "CODES = {'<PAD>': 0, '<EOS>': 1, '<UNK>': 2, '<GO>': 3 }\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    # make a list of unique words\n",
    "    vocab = set(text.split())\n",
    "\n",
    "    # (1)\n",
    "    # starts with the special tokens\n",
    "    vocab_to_int = copy.copy(CODES)\n",
    "\n",
    "    # the index (v_i) will starts from 4 (the 2nd arg in enumerate() specifies the starting index)\n",
    "    # since vocab_to_int already contains special tokens\n",
    "    for v_i, v in enumerate(vocab, len(CODES)):\n",
    "        vocab_to_int[v] = v_i\n",
    "\n",
    "    # (2)\n",
    "    int_to_vocab = {v_i: v for v, v_i in vocab_to_int.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "def text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "        1st, 2nd args: raw string text to be converted\n",
    "        3rd, 4th args: lookup tables for 1st and 2nd args respectively\n",
    "    \n",
    "        return: A tuple of lists (source_id_text, target_id_text) converted\n",
    "    \"\"\"\n",
    "    # empty list of converted sentences\n",
    "    source_text_id = []\n",
    "    target_text_id = []\n",
    "    \n",
    "    # make a list of sentences (extraction)\n",
    "    source_sentences = source_text.split(\"\\n\")\n",
    "    target_sentences = target_text.split(\"\\n\")\n",
    "    \n",
    "    max_source_sentence_length = max([len(sentence.split(\" \")) for sentence in source_sentences])\n",
    "    max_target_sentence_length = max([len(sentence.split(\" \")) for sentence in target_sentences])\n",
    "    \n",
    "    # iterating through each sentences (# of sentences in source&target is the same)\n",
    "    for i in range(len(source_sentences)):\n",
    "        # extract sentences one by one\n",
    "        source_sentence = source_sentences[i]\n",
    "        target_sentence = target_sentences[i]\n",
    "        \n",
    "        # make a list of tokens/words (extraction) from the chosen sentence\n",
    "        source_tokens = source_sentence.split()\n",
    "        target_tokens = target_sentence.split()\n",
    "        \n",
    "        # empty list of converted words to index in the chosen sentence\n",
    "        source_token_id = []\n",
    "        target_token_id = []\n",
    "        \n",
    "        for index, token in enumerate(source_tokens):\n",
    "            if (token != \"\"):\n",
    "                source_token_id.append(source_vocab_to_int[token])\n",
    "        \n",
    "        for index, token in enumerate(target_tokens):\n",
    "            if (token != \"\"):\n",
    "                target_token_id.append(target_vocab_to_int[token])\n",
    "                \n",
    "        # put <EOS> token at the end of the chosen target sentence\n",
    "        # this token suggests when to stop creating a sequence\n",
    "        target_token_id.append(target_vocab_to_int['<EOS>'])\n",
    "            \n",
    "        # add each converted sentences in the final list\n",
    "        source_text_id.append(source_token_id)\n",
    "        target_text_id.append(target_token_id)\n",
    "    \n",
    "    return source_text_id, target_text_id\n",
    "\n",
    "def preprocess_and_save_data(source_path, target_path, text_to_ids):\n",
    "    # Preprocess\n",
    "    \n",
    "    # load original data (English, French)\n",
    "    source_text = load_data(source_path)\n",
    "    target_text = load_data(target_path)\n",
    "\n",
    "    # to the lower case\n",
    "    source_text = source_text.lower()\n",
    "    target_text = target_text.lower()\n",
    "\n",
    "    # create lookup tables for English and French data\n",
    "    source_vocab_to_int, source_int_to_vocab = create_lookup_tables(source_text)\n",
    "    target_vocab_to_int, target_int_to_vocab = create_lookup_tables(target_text)\n",
    "\n",
    "    # create list of sentences whose words are represented in index\n",
    "    source_text, target_text = text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int)\n",
    "\n",
    "    # Save data for later use\n",
    "    pickle.dump((\n",
    "        (source_text, target_text),\n",
    "        (source_vocab_to_int, target_vocab_to_int),\n",
    "        (source_int_to_vocab, target_int_to_vocab)), open('preprocess.p', 'wb'))\n",
    "\n",
    "preprocess_and_save_data(source_path, target_path, text_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
